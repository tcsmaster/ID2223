{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning the Whisper model for swedish audio transcription\n",
        "\n",
        "This is part 1 of my Assignment in ID2223 Scalable Machine Learning and Deep Learning @ KTH\n",
        "\n",
        "The assignment was to fine-tune the [Whisper](https://openai.com/blog/whisper/) model for swedish audio transciption. The task was broken done into 3 parts:\n",
        "\n",
        "- feature pipeline: create features as inputs to the network, and store these features\n",
        "- training pipeline: train the model on the data\n",
        "- create an interface through which people can interact with the model\n",
        "\n",
        "For part 2, check out [this notebook](https://github.com/tcsmaster/ID2223/tree/main/Lab2/swedish-fine-tune-training-pipeline.ipynb)"
      ],
      "metadata": {
        "id": "c_9cGzUz2T9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "\n",
        "In this notebook:\n",
        "- I acquire the data for fine-tuning: the swedish data from the Common Voice dataset, which can be found [here](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/sv-SE/train)\n",
        "- I prepare the data using built-in methods from Huggingface:\n",
        "   - feature extractor\n",
        "   - tokenizer\n",
        "   - processor, which combines the two above\n",
        "- I store the data in Google Drive\n",
        "\n",
        "Note: this notebook is an adaptation of Sanchit Gandhi's [Fine-Tune Whisper for Multilingual ASR with ðŸ¤— Transformers](https://huggingface.co/blog/fine-tune-whisper)"
      ],
      "metadata": {
        "id": "DSYdTIqdrwHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg"
      ],
      "metadata": {
        "id": "i7GG9siAh20U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing some Python packages"
      ],
      "metadata": {
        "id": "Mf3iTcg-N1Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa"
      ],
      "metadata": {
        "id": "dUqXazdJiAo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can download and prepare the Common Voice splits in just one line of code.\n",
        "\n",
        "First, ensure you have accepted the terms of use on the [Hugging Face Hub](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally."
      ],
      "metadata": {
        "id": "trilgk2r296a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "mkNVXfCniObM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, Audio\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"train\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sv-SE\", split=\"test\", use_auth_token=True)"
      ],
      "metadata": {
        "id": "2WUSU4yUifF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we remove the unnecessary metadata columns from the dataset, as we only need the audio and the transcribed text."
      ],
      "metadata": {
        "id": "vwLHYPPo7TpU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIHTuhIuaz2A"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ASR pipeline can be decomposed into three stages: \n",
        "- A feature extractor which pre-processes the raw audio inputs\n",
        "- The model which performs the sequence-to-sequence mapping \n",
        "- A tokenizer which post-processes the model outputs to text format\n",
        "\n",
        "The Whisper model has an associated feature extractor and tokenizer, called WhisperFeatureExtractor and WhisperTokenizer respectively.\n",
        "\n",
        "The Whisper feature extractor performs two operations:\n",
        "\n",
        "- Pads / truncates the audio inputs to 30s: any audio inputs shorter than 30s are padded to 30s with silence (zeros), and those longer that 30s are truncated to 30s\n",
        "- Converts the audio inputs to log-Mel spectrogram input features, a visual representation of the audio and the form of the input expected by the Whisper model\n",
        "\n",
        "The Whisper model outputs a sequence of token ids. The tokenizer maps each of these token ids to their corresponding text string. For Hindi, we can load the pre-trained tokenizer and use it for fine-tuning without any further modifications. We simply have to specify the target language and the task. These arguments inform the tokenizer to prefix the language and task tokens to the start of encoded label sequences.\n",
        "\n",
        "To simplify using the feature extractor and tokenizer, we can _wrap_ \n",
        "both into a single `WhisperProcessor` class. This processor object \n",
        "inherits from the `WhisperFeatureExtractor` and `WhisperProcessor`, \n",
        "and can be used on the audio inputs and model predictions as required. \n",
        "In doing so, we only need to keep track of two objects during training: \n",
        "the `processor` and the `model`:"
      ],
      "metadata": {
        "id": "YcMbI_WT-Ig-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Swedish\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Swedish\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "8kTFOV4NhUBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, since the feature extractor expects the udio to be sampled at 16 kHz, we resample the audio."
      ],
      "metadata": {
        "id": "c6EQsRJD8rQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "LVn3Wjou8qO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define a batch-preprocessor, which resamples the audio at 16 kHz, applies the featue extractor on the inputs and the tokenizer on the labels."
      ],
      "metadata": {
        "id": "8cMj7REXDhfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array \n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids \n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "8z0gmZDvhIi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=3)"
      ],
      "metadata": {
        "id": "JohtINy5hJY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last, but not least, we need to store our data somewhere. There are a lot of possibilities to do that, for this, we're going to use Google Drive."
      ],
      "metadata": {
        "id": "5XFSxApNEtaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "common_voice.save_to_disk(\"common_voice\")\n",
        "os.mkdir(\"/content/gdrive/My Drive/common_voice\")\n",
        "common_voice.save_to_disk(F\"/content/gdrive/My Drive/common_voice/\")"
      ],
      "metadata": {
        "id": "6dTs8xNchWp-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}